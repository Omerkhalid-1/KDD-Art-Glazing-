{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9092059,"sourceType":"datasetVersion","datasetId":5486644},{"sourceId":9092064,"sourceType":"datasetVersion","datasetId":5486649},{"sourceId":9092066,"sourceType":"datasetVersion","datasetId":5486651},{"sourceId":9103855,"sourceType":"datasetVersion","datasetId":5494437},{"sourceId":9103878,"sourceType":"datasetVersion","datasetId":5494454}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport numpy as np\nimport time\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom PIL import Image\nimport random\nimport matplotlib.pyplot as plt\nimport os\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Set the base path for your project in Kaggle\nbase_path = '/kaggle/working/YourProjectFolder'  # Change this to your actual project folder path\n\nclass SimpleUNet(nn.Module):\n    def __init__(self):\n        super(SimpleUNet, self).__init__()\n        # Encoder\n        self.enc1 = self.conv_block(3, 64)\n        self.enc2 = self.conv_block(64, 128)\n        self.enc3 = self.conv_block(128, 256)\n        self.enc4 = self.conv_block(256, 512)\n\n        # Decoder\n        self.dec1 = self.conv_block(512, 256)\n        self.dec2 = self.conv_block(512, 128)\n        self.dec3 = self.conv_block(256, 64)\n        self.dec4 = nn.Conv2d(128, 3, kernel_size=3, padding=1)\n\n    def conv_block(self, in_ch, out_ch):\n        return nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x, t):\n        # Encoder\n        e1 = self.enc1(x)\n        e2 = self.enc2(F.max_pool2d(e1, 2))\n        e3 = self.enc3(F.max_pool2d(e2, 2))\n        e4 = self.enc4(F.max_pool2d(e3, 2))\n\n        # Decoder\n        d1 = self.dec1(F.interpolate(e4, size=e3.shape[2:], mode='nearest'))\n        d2 = self.dec2(torch.cat([d1, e3], dim=1))\n        d3 = self.dec3(torch.cat([F.interpolate(d2, size=e2.shape[2:], mode='nearest'), e2], dim=1))\n        output = self.dec4(torch.cat([F.interpolate(d3, size=e1.shape[2:], mode='nearest'), e1], dim=1))\n\n        return output\n\nclass ImageDataset(Dataset):\n    def __init__(self, folder_path, small_images_folder, transform=None):\n        self.folder_path = folder_path\n        self.image_files = [f for f in os.listdir(folder_path) if f.endswith(('jpg', 'png', 'jpeg'))]\n        self.small_images = [Image.open(os.path.join(small_images_folder, f)) for f in os.listdir(small_images_folder) if f.endswith(('jpg', 'png', 'jpeg'))]\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.folder_path, self.image_files[idx])\n        image = Image.open(img_path)\n\n        # Resize image to ensure dimensions are divisible by 8\n        w, h = image.size\n        new_w, new_h = ((w + 7) // 8) * 8, ((h + 7) // 8) * 8\n        image = image.resize((new_w, new_h), Image.LANCZOS)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image\n\ndef place_image(main_img, small_img):\n    # Convert numpy arrays to PIL Images\n    main_img_pil = Image.fromarray((main_img * 255).astype(np.uint8))\n    small_img_pil = small_img.convert(\"RGBA\")  # Ensure small_img is in RGBA mode\n\n    # Resize small image to be smaller if necessary\n    small_img_pil.thumbnail((main_img_pil.width // 4, main_img_pil.height // 4), Image.LANCZOS)\n\n    # Get dimensions\n    main_width, main_height = main_img_pil.size\n    small_width, small_height = small_img_pil.size\n\n    # Generate random position\n    x = random.randint(0, main_width - small_width)\n    y = random.randint(0, main_height - small_height)\n\n    # Adjust opacity\n    datas = small_img_pil.getdata()\n    new_data = []\n    for item in datas:\n        # Change all white (also shades of whites) pixels to transparent\n        if item[0] in list(range(200, 256)):\n            new_data.append((255, 255, 255, 0))\n        else:\n            new_data.append((item[0], item[1], item[2], 10))  # Adjust alpha value (0-255) for lower opacity\n    small_img_pil.putdata(new_data)\n\n    # Paste the image onto the main image\n    main_img_pil.paste(small_img_pil, (x, y), small_img_pil)\n\n    # Convert back to numpy array\n    return np.array(main_img_pil) / 255.0\n\ndef add_image_noise(image, small_images, t, max_t):\n    # Number of small images to paste based on timestep\n    num_images = int((t / max_t) * 300)  # Adjust 300 as needed\n\n    noisy_image = image.copy()\n    for _ in range(num_images):\n        small_img = random.choice(small_images)\n        noisy_image = place_image(noisy_image, small_img)\n\n    return torch.from_numpy(noisy_image).permute(2, 0, 1).unsqueeze(0).float()\n\ndef show_images(noisy_image, denoised_image, title):\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.imshow(noisy_image)\n    plt.title('Noisy Image')\n    plt.axis('off')\n\n    denoised_image_path = os.path.join(base_path, f\"{title}.PNG\")\n    denoised_image.save(denoised_image_path)\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(denoised_image)\n    plt.title('Denoised Image')\n    plt.axis('off')\n\n    plt.suptitle(title)\n    plt.savefig(os.path.join(base_path, f\"{title}_comparison.png\"))\n    plt.close()\n\ndef train_ddpm(model, optimizer, train_loader, val_loader, small_images, num_timesteps, num_epochs, device):\n    model.to(device)\n    best_val_loss = float('inf')\n    total_start_time = time.time()\n    model_save_path = os.path.join(base_path, 'best_model.pth')\n    writer = SummaryWriter(os.path.join(base_path, 'logs'))\n\n    for epoch in range(num_epochs):\n        epoch_start_time = time.time()\n        model.train()\n        train_loss = 0\n\n        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n            main_image = batch.to(device)\n\n            for t in range(num_timesteps):\n                optimizer.zero_grad()\n\n                # Add image noise\n                noisy_image = add_image_noise(main_image.cpu().squeeze(0).permute(1, 2, 0).numpy(), small_images, t, num_timesteps)\n                noisy_image = noisy_image.to(device)\n\n                # Predict denoised image\n                predicted = model(noisy_image, torch.tensor([t]).float().to(device))\n\n                # Calculate loss\n                loss = F.mse_loss(predicted, main_image)\n\n                loss.backward()\n                optimizer.step()\n\n                train_loss += loss.item()\n\n            # Print progress for every batch\n            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n\n        # Validation\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                main_image = batch.to(device)\n\n                for t in range(num_timesteps):\n                    noisy_image = add_image_noise(main_image.cpu().squeeze(0).permute(1, 2, 0).numpy(), small_images, t, num_timesteps)\n                    noisy_image = noisy_image.to(device)\n\n                    predicted = model(noisy_image, torch.tensor([t]).float().to(device))\n                    loss = F.mse_loss(predicted, main_image)\n                    val_loss += loss.item()\n\n        train_loss /= len(train_loader) * num_timesteps\n        val_loss /= len(val_loader) * num_timesteps\n\n        # Log metrics\n        writer.add_scalar('Loss/train', train_loss, epoch)\n        writer.add_scalar('Loss/val', val_loss, epoch)\n\n        epoch_end_time = time.time()\n        epoch_duration = epoch_end_time - epoch_start_time\n        total_duration = epoch_end_time - total_start_time\n        estimated_time_left = (num_epochs - epoch - 1) * epoch_duration\n\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n        print(f\"Epoch duration: {epoch_duration:.2f} seconds\")\n        print(f\"Total duration: {total_duration:.2f} seconds\")\n        print(f\"Estimated time left: {estimated_time_left:.2f} seconds\")\n\n        # Save the best model\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), model_save_path)\n\n    print(f\"Training completed in {time.time() - total_start_time:.2f} seconds\")\n\n    # Close the SummaryWriter\n    writer.close()\n\ndef inference(model, device, image_folder, small_images, num_timesteps):\n    model.to(device)\n    model.eval()\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n    ])\n\n    dataset = ImageDataset(image_folder, small_images, transform=transform)\n    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n\n    with torch.no_grad():\n        for i, main_image in enumerate(dataloader):\n            main_image = main_image.to(device)\n\n            noisy_image = main_image\n            for t in reversed(range(num_timesteps)):\n                noisy_image = add_image_noise(noisy_image.cpu().squeeze(0).permute(1, 2, 0).numpy(), small_images, t, num_timesteps)\n                noisy_image = noisy_image.to(device)\n                predicted = model(noisy_image, torch.tensor([t]).float().to(device))\n                noisy_image = predicted\n\n            # Convert tensors to PIL Images for saving\n            noisy_image_pil = transforms.ToPILImage()(noisy_image.squeeze(0).cpu())\n            denoised_image_pil = transforms.ToPILImage()(main_image.squeeze(0).cpu())\n\n            show_images(noisy_image_pil, denoised_image_pil, f'inference_result_{i}')\n\n# Set the number of timesteps and epochs\nnum_timesteps = 10  # Set based on your requirements\nnum_epochs = 5  # Set based on your requirements\n\n# Set the folder paths\ntrain_folder = '/kaggle/input/train-f1'\nval_folder = '/kaggle/input/validation1'\nsmall_images_folder = '/kaggle/input/small-images1'\noutput_dir = os.path.join(base_path, 'output')\n# Define transformations\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n])\n\n# Create datasets and dataloaders\ntrain_dataset = ImageDataset(train_folder, small_images_folder, transform=transform)\nval_dataset = ImageDataset(val_folder, small_images_folder, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n\n# Instantiate the model and optimizer\nmodel = SimpleUNet()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n\n# Get the small images for adding noise\nsmall_images = [Image.open(os.path.join(small_images_folder, f)) for f in os.listdir(small_images_folder) if f.endswith(('jpg', 'png', 'jpeg'))]\n\n\ntrain_ddpm(model, optimizer, train_loader, val_loader, small_images, num_timesteps, num_epochs, device='cuda' if torch.cuda.is_available() else 'cpu')\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-02T20:45:40.938805Z","iopub.execute_input":"2024-08-02T20:45:40.939186Z","iopub.status.idle":"2024-08-02T22:11:47.493087Z","shell.execute_reply.started":"2024-08-02T20:45:40.939154Z","shell.execute_reply":"2024-08-02T22:11:47.491979Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Epoch 1/5:  20%|██        | 1/5 [01:43<06:52, 103.07s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Batch 1/5, Loss: 0.1768\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5:  40%|████      | 2/5 [03:27<05:11, 103.90s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Batch 2/5, Loss: 0.0143\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5:  60%|██████    | 3/5 [05:11<03:27, 103.78s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Batch 3/5, Loss: 0.0276\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5:  80%|████████  | 4/5 [06:55<01:43, 103.94s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Batch 4/5, Loss: 0.0103\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 5/5 [08:34<00:00, 102.83s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5, Batch 5/5, Loss: 0.0188\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5\nTrain Loss: 0.0749, Val Loss: 0.0331\nEpoch duration: 1033.24 seconds\nTotal duration: 1033.24 seconds\nEstimated time left: 4132.96 seconds\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5:  20%|██        | 1/5 [01:39<06:39, 99.96s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Batch 1/5, Loss: 0.0064\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5:  40%|████      | 2/5 [03:18<04:57, 99.01s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Batch 2/5, Loss: 0.0184\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5:  60%|██████    | 3/5 [05:01<03:22, 101.09s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Batch 3/5, Loss: 0.0063\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5:  80%|████████  | 4/5 [06:43<01:41, 101.41s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Batch 4/5, Loss: 0.0087\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5: 100%|██████████| 5/5 [08:21<00:00, 100.32s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5, Batch 5/5, Loss: 0.0141\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5\nTrain Loss: 0.0118, Val Loss: 0.0160\nEpoch duration: 1013.43 seconds\nTotal duration: 2046.71 seconds\nEstimated time left: 3040.28 seconds\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5:  20%|██        | 1/5 [01:38<06:32, 98.17s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Batch 1/5, Loss: 0.0044\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5:  40%|████      | 2/5 [03:23<05:06, 102.28s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Batch 2/5, Loss: 0.0125\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5:  60%|██████    | 3/5 [05:10<03:29, 104.66s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Batch 3/5, Loss: 0.0055\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5:  80%|████████  | 4/5 [06:59<01:46, 106.38s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Batch 4/5, Loss: 0.0119\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5: 100%|██████████| 5/5 [08:48<00:00, 105.60s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5, Batch 5/5, Loss: 0.0162\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5\nTrain Loss: 0.0082, Val Loss: 0.0143\nEpoch duration: 1056.48 seconds\nTotal duration: 3103.27 seconds\nEstimated time left: 2112.97 seconds\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5:  20%|██        | 1/5 [01:45<07:02, 105.51s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Batch 1/5, Loss: 0.0061\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5:  40%|████      | 2/5 [03:32<05:19, 106.67s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Batch 2/5, Loss: 0.0114\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5:  60%|██████    | 3/5 [05:21<03:34, 107.46s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Batch 3/5, Loss: 0.0078\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5:  80%|████████  | 4/5 [07:00<01:44, 104.27s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Batch 4/5, Loss: 0.0136\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5: 100%|██████████| 5/5 [08:44<00:00, 104.82s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5, Batch 5/5, Loss: 0.0044\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5\nTrain Loss: 0.0070, Val Loss: 0.0125\nEpoch duration: 1052.65 seconds\nTotal duration: 4155.99 seconds\nEstimated time left: 1052.65 seconds\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5:  20%|██        | 1/5 [01:49<07:19, 109.79s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Batch 1/5, Loss: 0.0126\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5:  40%|████      | 2/5 [03:32<05:17, 105.90s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Batch 2/5, Loss: 0.0068\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5:  60%|██████    | 3/5 [05:18<03:31, 105.51s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Batch 3/5, Loss: 0.0038\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5:  80%|████████  | 4/5 [06:56<01:42, 102.84s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Batch 4/5, Loss: 0.0053\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5: 100%|██████████| 5/5 [08:34<00:00, 102.89s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5, Batch 5/5, Loss: 0.0134\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5\nTrain Loss: 0.0059, Val Loss: 0.0118\nEpoch duration: 1010.27 seconds\nTotal duration: 5166.32 seconds\nEstimated time left: 0.00 seconds\nTraining completed in 5166.38 seconds\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Code For Inference: ","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport time\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom PIL import Image\nimport random\nimport matplotlib.pyplot as plt\nimport os\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nbase_path = '/kaggle/working'\ndef show_images(noisy_image, denoised_image, title):\n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 2, 1)\n    plt.imshow(noisy_image)\n    plt.title('Noisy Image')\n    plt.axis('off')\n\n    denoised_image_path = os.path.join(base_path, f\"{title}.PNG\")\n    denoised_image.save(denoised_image_path)\n\n    plt.subplot(1, 2, 2)\n    plt.imshow(denoised_image)\n    plt.title('Denoised Image')\n    plt.axis('off')\n\n    plt.suptitle(title)\n    plt.savefig(os.path.join(base_path, f\"{title}_comparison.png\"))\n    plt.close()\n\n\n\ndef add_image_noise(image, small_images, t, max_t):\n    # Number of small images to paste based on timestep\n    num_images = int((t / max_t) * 300)  # Adjust 300 as needed\n\n    noisy_image = image.copy()\n    for _ in range(num_images):\n        small_img = random.choice(small_images)\n        noisy_image = place_image(noisy_image, small_img)\n\n    return torch.from_numpy(noisy_image).permute(2, 0, 1).unsqueeze(0).float()\n\n\n\nclass ImageDataset(Dataset):\n    def __init__(self, folder_path, small_images_folder, transform=None):\n        self.folder_path = folder_path\n        self.image_files = [f for f in os.listdir(folder_path) if f.endswith(('jpg', 'png', 'jpeg'))]\n        self.small_images = [Image.open(os.path.join(small_images_folder, f)) for f in os.listdir(small_images_folder) if f.endswith(('jpg', 'png', 'jpeg'))]\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.folder_path, self.image_files[idx])\n        image = Image.open(img_path)\n\n        # Resize image to ensure dimensions are divisible by 8\n        w, h = image.size\n        new_w, new_h = ((w + 7) // 8) * 8, ((h + 7) // 8) * 8\n        image = image.resize((new_w, new_h), Image.LANCZOS)\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image\n\n    \ndef place_image(main_img, small_img):\n    # Convert numpy arrays to PIL Images\n    main_img_pil = Image.fromarray((main_img * 255).astype(np.uint8))\n    small_img_pil = small_img.convert(\"RGBA\")  # Ensure small_img is in RGBA mode\n\n    # Resize small image to be smaller if necessary\n    small_img_pil.thumbnail((main_img_pil.width // 4, main_img_pil.height // 4), Image.LANCZOS)\n\n    # Get dimensions\n    main_width, main_height = main_img_pil.size\n    small_width, small_height = small_img_pil.size\n\n    # Generate random position\n    x = random.randint(0, main_width - small_width)\n    y = random.randint(0, main_height - small_height)\n\n    # Adjust opacity\n    datas = small_img_pil.getdata()\n    new_data = []\n    for item in datas:\n        # Change all white (also shades of whites) pixels to transparent\n        if item[0] in list(range(200, 256)):\n            new_data.append((255, 255, 255, 0))\n        else:\n            new_data.append((item[0], item[1], item[2], 10))  # Adjust alpha value (0-255) for lower opacity\n    small_img_pil.putdata(new_data)\n\n    # Paste the image onto the main image\n    main_img_pil.paste(small_img_pil, (x, y), small_img_pil)\n\n    # Convert back to numpy array\n    return np.array(main_img_pil) / 255.0\n\ndef inference(model, device, image_folder, small_images_folder, num_timesteps):\n    model.to(device)\n    model.eval()\n\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n    ])\n\n    dataset = ImageDataset(image_folder, small_images_folder, transform=transform)\n    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n\n    # Load small images here\n    small_images = [Image.open(os.path.join(small_images_folder, f)) for f in os.listdir(small_images_folder) if f.endswith(('jpg', 'png', 'jpeg'))]\n\n    with torch.no_grad():\n        for i, main_image in enumerate(dataloader):\n            main_image = main_image.to(device)\n\n            noisy_image = main_image\n            for t in reversed(range(num_timesteps)):\n                noisy_image = add_image_noise(noisy_image.cpu().squeeze(0).permute(1, 2, 0).numpy(), small_images, t, num_timesteps)\n                noisy_image = noisy_image.to(device)\n                predicted = model(noisy_image, torch.tensor([t]).float().to(device))\n                noisy_image = predicted\n\n            # Convert tensors to PIL Images for saving\n            noisy_image_pil = transforms.ToPILImage()(noisy_image.squeeze(0).cpu())\n            denoised_image_pil = transforms.ToPILImage()(main_image.squeeze(0).cpu())\n\n            show_images(noisy_image_pil, denoised_image_pil, f'inference_result_{i}')\n\n\nclass SimpleUNet(nn.Module):\n    def __init__(self):\n        super(SimpleUNet, self).__init__()\n        # Encoder\n        self.enc1 = self.conv_block(3, 64)\n        self.enc2 = self.conv_block(64, 128)\n        self.enc3 = self.conv_block(128, 256)\n        self.enc4 = self.conv_block(256, 512)\n\n        # Decoder\n        self.dec1 = self.conv_block(512, 256)\n        self.dec2 = self.conv_block(512, 128)\n        self.dec3 = self.conv_block(256, 64)\n        self.dec4 = nn.Conv2d(128, 3, kernel_size=3, padding=1)\n\n    def conv_block(self, in_ch, out_ch):\n        return nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x, t):\n        # Encoder\n        e1 = self.enc1(x)\n        e2 = self.enc2(F.max_pool2d(e1, 2))\n        e3 = self.enc3(F.max_pool2d(e2, 2))\n        e4 = self.enc4(F.max_pool2d(e3, 2))\n\n        # Decoder\n        d1 = self.dec1(F.interpolate(e4, size=e3.shape[2:], mode='nearest'))\n        d2 = self.dec2(torch.cat([d1, e3], dim=1))\n        d3 = self.dec3(torch.cat([F.interpolate(d2, size=e2.shape[2:], mode='nearest'), e2], dim=1))\n        output = self.dec4(torch.cat([F.interpolate(d3, size=e1.shape[2:], mode='nearest'), e1], dim=1))\n\n        return output\n\n\n\n\ninference_folder = '/kaggle/input/inference'  # Replace with your inference images folder\nsmall_images_folder = '/kaggle/input/small-images1'\nmodel_path = '/kaggle/input/model-best/best_model (2).pth'\n\nmodel = SimpleUNet()\nmodel.load_state_dict(torch.load(model_path))\n\n# Set device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# Get small images for adding noise\nsmall_images = [Image.open(os.path.join(small_images_folder, f)) for f in os.listdir(small_images_folder) if f.endswith(('jpg', 'png', 'jpeg'))]\n\n# Set device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nnum_timesteps = 10  # Set based on your requirements\nnum_epochs = 5  # Set based on your requirements\n\n# Run inference\ninference(model, device, inference_folder, small_images_folder, num_timesteps)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-04T15:51:41.286728Z","iopub.execute_input":"2024-08-04T15:51:41.287145Z","iopub.status.idle":"2024-08-04T16:02:05.738970Z","shell.execute_reply.started":"2024-08-04T15:51:41.287114Z","shell.execute_reply":"2024-08-04T16:02:05.738166Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}