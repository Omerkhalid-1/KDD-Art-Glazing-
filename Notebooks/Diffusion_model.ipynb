{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SDyjAKYIaqw"
      },
      "source": [
        "## Diffusion Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjTwtaAatPx8"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpVD-RrwWofw",
        "outputId": "1b54a439-2053-4265-9699-276c98d9f079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, img_dir, transform=None, target_width=800, target_height=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.img_paths = [os.path.join(img_dir, img_name) for img_name in os.listdir(img_dir)]\n",
        "        self.transform = transform\n",
        "        self.target_width = target_width\n",
        "        self.target_height = target_height\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        aspect_ratio = image.height / image.width\n",
        "        new_height = int(self.target_width * aspect_ratio)\n",
        "\n",
        "        # Apply the resizing transform with the calculated new size\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((new_height, self.target_width), interpolation=Image.LANCZOS),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "        image = transform(image)\n",
        "        if self.target_height:\n",
        "            padding = (0, 0, 0, self.target_height - new_height)  # left, top, right, bottom\n",
        "            image = F.pad(image, padding, \"constant\", 0)\n",
        "\n",
        "        return image\n",
        "\n",
        "target_width = 1024\n",
        "target_height = 1024\n",
        "img_dir = '/content/drive/MyDrive/Input'\n",
        "\n",
        "dataset = CustomImageDataset(img_dir, target_width=target_width, target_height=target_height)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzIt6D3kOBXN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zszDREKAWx4X",
        "outputId": "5546aaee-39e0-4a01-8d0e-05c3082b78a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved original image to /content/drive/MyDrive/enchance/epoch_4_batch_0_original.png\n",
            "Saved enhanced image to /content/drive/MyDrive/enchance/epoch_4_batch_0_enhanced.png\n",
            "Epoch 5, Average Loss: 0.3744173910130154\n",
            "Saved original image to /content/drive/MyDrive/enchance/epoch_9_batch_0_original.png\n",
            "Saved enhanced image to /content/drive/MyDrive/enchance/epoch_9_batch_0_enhanced.png\n",
            "Epoch 10, Average Loss: 0.3709434744986621\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import os\n",
        "import torchvision.transforms\n",
        "import torch.quantization as quantization\n",
        "\n",
        "# Define the Block, Encoder, Decoder, and UNet classes as before\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, chs=(3,64,128,256,512,1024)):\n",
        "        super().__init__()\n",
        "        self.enc_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
        "        self.pool       = nn.MaxPool2d(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ftrs = []\n",
        "        for block in self.enc_blocks:\n",
        "            x = block(x)\n",
        "            ftrs.append(x)\n",
        "            x = self.pool(x)\n",
        "        return ftrs\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, chs=(1024, 512, 256, 128, 64)):\n",
        "        super().__init__()\n",
        "        self.chs         = chs\n",
        "        self.upconvs    = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])\n",
        "        self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
        "\n",
        "    def forward(self, x, encoder_features):\n",
        "        for i in range(len(self.chs)-1):\n",
        "            x        = self.upconvs[i](x)\n",
        "            enc_ftrs = self.crop(encoder_features[i], x)\n",
        "            x        = torch.cat([x, enc_ftrs], dim=1)\n",
        "            x        = self.dec_blocks[i](x)\n",
        "        return x\n",
        "\n",
        "    def crop(self, enc_ftrs, x):\n",
        "        _, _, H, W = x.shape\n",
        "        enc_ftrs   = torchvision.transforms.CenterCrop([H, W])(enc_ftrs)\n",
        "        return enc_ftrs\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, enc_chs=(3,64,128,256,512,1024), dec_chs=(1024, 512, 256, 128, 64), num_class=3, retain_dim=False, out_sz=(572,572)):\n",
        "        super().__init__()\n",
        "        self.encoder     = Encoder(enc_chs)\n",
        "        self.decoder     = Decoder(dec_chs)\n",
        "        self.head        = nn.Conv2d(dec_chs[-1], num_class, 1)\n",
        "        self.retain_dim  = retain_dim\n",
        "        self.out_sz      = out_sz\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_ftrs = self.encoder(x)\n",
        "        out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
        "        out      = self.head(out)\n",
        "        if self.retain_dim:\n",
        "            out = F.interpolate(out, self.out_sz)\n",
        "        return out\n",
        "\n",
        "class ComplexDiffusionModel(nn.Module):\n",
        "    def __init__(self, timesteps=10, beta_start=0.0001, beta_end=0.0005):\n",
        "        super(ComplexDiffusionModel, self).__init__()\n",
        "        self.timesteps = timesteps\n",
        "\n",
        "        # Define the variance schedule (beta)\n",
        "        self.betas = torch.linspace(beta_start, beta_end, timesteps).to(device)\n",
        "\n",
        "        # Precompute alpha and alpha_cumprod\n",
        "        self.alphas = (1.0 - self.betas).to(device)\n",
        "        self.alpha_cumprod = torch.cumprod(self.alphas, dim=0).to(device)\n",
        "\n",
        "        # Define the UNet model\n",
        "        self.unet = UNet().to(device)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        return self.unet(x)\n",
        "\n",
        "    def forward_process(self, x0, t):\n",
        "        alpha_t = self.alpha_cumprod[t].view(-1, 1, 1, 1)\n",
        "        std_dev = torch.sqrt(1.0 - alpha_t)\n",
        "        noise = torch.randn_like(x0).to(device) * 0.5  # Scaled down noise\n",
        "        x_t = alpha_t.sqrt() * x0 + std_dev * noise\n",
        "        return x_t, noise\n",
        "\n",
        "    def reverse_process(self, xt, t):\n",
        "        predicted_noise = self.forward(xt, t)\n",
        "        alpha_t = self.alpha_cumprod[t].view(-1, 1, 1, 1)\n",
        "        alpha_prev = self.alpha_cumprod[t - 1].view(-1, 1, 1, 1)\n",
        "\n",
        "        xt_predicted = (xt - torch.sqrt(1.0 - alpha_t) * predicted_noise) / torch.sqrt(alpha_t)\n",
        "        x_prev = torch.sqrt(alpha_prev) * xt_predicted + torch.sqrt(1 - alpha_prev) * predicted_noise\n",
        "\n",
        "        return x_prev\n",
        "\n",
        "# Helper function to convert tensor to PIL image\n",
        "def tensor_to_pil_image(tensor):\n",
        "    tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min())  # Normalize to [0, 1]\n",
        "    tensor = tensor.mul(255).byte()\n",
        "    tensor = tensor.permute(1, 2, 0).cpu().numpy()\n",
        "    return Image.fromarray(tensor)\n",
        "\n",
        "# Training function\n",
        "def train(model, dataloader, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for batch_idx, x0 in enumerate(dataloader):\n",
        "            x0 = x0.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            t = torch.randint(0, model.timesteps, (x0.size(0),), device=x0.device)\n",
        "            xt, noise = model.forward_process(x0, t)\n",
        "            predicted_noise = model.reverse_process(xt, t)\n",
        "            loss = loss_fn(noise, predicted_noise)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate the loss for this epoch\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Save the original and enhanced images\n",
        "            if (epoch + 1) % 1 == 0 and batch_idx == 0:\n",
        "                with torch.no_grad():\n",
        "                    model.eval()\n",
        "                    x0_sample = tensor_to_pil_image(x0[0])\n",
        "                    xt_sample = tensor_to_pil_image(xt[0])\n",
        "\n",
        "                    original_image_path = os.path.join(enhance_folder, f'epoch_{epoch}_batch_{batch_idx}_original.png')\n",
        "                    enhanced_image_path = os.path.join(enhance_folder, f'epoch_{epoch}_batch_{batch_idx}_enhanced.png')\n",
        "\n",
        "                    # Ensure the directory exists\n",
        "                    os.makedirs(enhance_folder, exist_ok=True)\n",
        "\n",
        "                    x0_sample.save(original_image_path)\n",
        "                    xt_sample.save(enhanced_image_path)\n",
        "\n",
        "                    print(f'Saved original image to {original_image_path}')\n",
        "                    print(f'Saved enhanced image to {enhanced_image_path}')\n",
        "\n",
        "        # Print the average loss for this epoch after every 20 epochs\n",
        "        if (epoch + 1) % 1 == 0:\n",
        "            avg_epoch_loss = epoch_loss / len(dataloader)\n",
        "            print(f'Epoch {epoch + 1}, Average Loss: {avg_epoch_loss}')\n",
        "\n",
        "# Usage\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = ComplexDiffusionModel().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_fn = nn.MSELoss()\n",
        "enhance_folder = '/content/drive/MyDrive/enchance'  # Change this to the path where you want to save the images\n",
        "\n",
        "# Assume `dataloader` is defined and provides data\n",
        "train(model, dataloader, epochs=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "MeS-bFAxhx0y",
        "outputId": "f03eb90a-050b-4b06-8c5f-1f93926e317a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'dataloader' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-85d056c31982>\u001b[0m in \u001b[0;36m<cell line: 83>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# Assume `dataloader` is defined and provides data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class NoiseRemovalModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NoiseRemovalModel, self).__init__()\n",
        "\n",
        "        # Define the neural network layers\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv5 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv6 = nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = F.relu(self.conv5(x))\n",
        "        x = self.conv6(x)\n",
        "        return x\n",
        "\n",
        "    def denoise(self, noisy_image):\n",
        "        # Pass the noisy image through the network to get the denoised output\n",
        "        denoised_image = self.forward(noisy_image)\n",
        "        return denoised_image\n",
        "\n",
        "# Helper function to convert tensor to PIL image\n",
        "def tensor_to_pil_image(tensor):\n",
        "    tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min())  # Normalize to [0, 1]\n",
        "    tensor = tensor.mul(255).byte()\n",
        "    tensor = tensor.permute(1, 2, 0).cpu().numpy()\n",
        "    return Image.fromarray(tensor)\n",
        "\n",
        "# Training function\n",
        "def train(model, dataloader, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for batch_idx, x0 in enumerate(dataloader):\n",
        "            x0 = x0.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Use the model to predict the clean image from the noisy input\n",
        "            predicted_clean_image = model.denoise(x0)\n",
        "\n",
        "            # The target is the original image (without added noise)\n",
        "            loss = loss_fn(predicted_clean_image, x0)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}')\n",
        "\n",
        "            # Save the original and denoised images\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                x0_sample = tensor_to_pil_image(x0[0])\n",
        "                denoised_sample = tensor_to_pil_image(predicted_clean_image[0])\n",
        "\n",
        "                original_image_path = os.path.join(enhance_folder, f'epoch_{epoch}_batch_{batch_idx}_original.png')\n",
        "                denoised_image_path = os.path.join(enhance_folder, f'epoch_{epoch}_batch_{batch_idx}_denoised.png')\n",
        "\n",
        "                # Ensure the directory exists\n",
        "                os.makedirs(enhance_folder, exist_ok=True)\n",
        "\n",
        "                x0_sample.save(original_image_path)\n",
        "                denoised_sample.save(denoised_image_path)\n",
        "\n",
        "                print(f'Saved original image to {original_image_path}')\n",
        "                print(f'Saved denoised image to {denoised_image_path}')\n",
        "\n",
        "# Usage\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = NoiseRemovalModel().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
        "loss_fn = nn.MSELoss()\n",
        "enhance_folder = '/content/drive/MyDrive/enchance'  # Change this to the path where you want to save the images\n",
        "\n",
        "# Assume `dataloader` is defined and provides data\n",
        "train(model, dataloader, epochs=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6ZrO3SiU3NU"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Example paths (replace with your actual paths)\n",
        "image_path = '/content/drive/MyDrive/Input/panda.jpg'\n",
        "output_path = '/content/drive/MyDrive/Input/Resized_Image.jpg'\n",
        "\n",
        "# Check if the file has a valid extension and exists\n",
        "if not image_path.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
        "    raise ValueError(f\"File extension is not supported: {image_path}\")\n",
        "\n",
        "# Load the image\n",
        "try:\n",
        "    image = Image.open(image_path)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading image: {e}\")\n",
        "    raise\n",
        "\n",
        "# Resize the image while maintaining the aspect ratio\n",
        "new_width = 800\n",
        "aspect_ratio = image.height / image.width\n",
        "new_height = int(new_width * aspect_ratio)\n",
        "\n",
        "# Apply the resizing transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((new_height, new_width), interpolation=Image.LANCZOS),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "transformed_image = transform(image)\n",
        "\n",
        "# Convert back to PIL image if needed and save\n",
        "resized_image = transforms.ToPILImage()(transformed_image)\n",
        "\n",
        "try:\n",
        "    resized_image.save(output_path, quality=95, subsampling=0)  # Save with high quality\n",
        "    print(f\"Resized image saved at: {output_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving image: {e}\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7lmv7X9p_V7",
        "outputId": "c3086e4d-62f7-4d8e-906f-0f2b48d68b68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved original image to /content/drive/MyDrive/enchance/epoch_19_batch_0_original.png\n",
            "Saved enhanced image to /content/drive/MyDrive/enchance/epoch_19_batch_0_enhanced.png\n",
            "Epoch 20, Average Loss: 0.2977733314037323\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, img_dir, transform=None, target_width=800):\n",
        "        self.img_dir = img_dir\n",
        "        self.img_paths = [os.path.join(img_dir, img_name) for img_name in os.listdir(img_dir)]\n",
        "        self.transform = transform\n",
        "        self.target_width = target_width\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Apply the resizing transform with the calculated new size\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((self.target_width, self.target_width), interpolation=Image.LANCZOS),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "        image = transform(image)\n",
        "        return image\n",
        "\n",
        "target_width = 800\n",
        "img_dir = '/content/drive/MyDrive/Input'\n",
        "\n",
        "dataset = CustomImageDataset(img_dir, target_width=target_width)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Define UNet model, training function, and other related code\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, chs=(3,64,128,256,512,1024)):\n",
        "        super().__init__()\n",
        "        self.enc_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
        "        self.pool       = nn.MaxPool2d(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ftrs = []\n",
        "        for block in self.enc_blocks:\n",
        "            x = block(x)\n",
        "            ftrs.append(x)\n",
        "            x = self.pool(x)\n",
        "        return ftrs\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, chs=(1024, 512, 256, 128, 64)):\n",
        "        super().__init__()\n",
        "        self.chs         = chs\n",
        "        self.upconvs    = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])\n",
        "        self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
        "\n",
        "    def forward(self, x, encoder_features):\n",
        "        for i in range(len(self.chs)-1):\n",
        "            x        = self.upconvs[i](x)\n",
        "            enc_ftrs = self.crop(encoder_features[i], x)\n",
        "            x        = torch.cat([x, enc_ftrs], dim=1)\n",
        "            x        = self.dec_blocks[i](x)\n",
        "        return x\n",
        "\n",
        "    def crop(self, enc_ftrs, x):\n",
        "        _, _, H, W = x.shape\n",
        "        enc_ftrs   = torchvision.transforms.CenterCrop([H, W])(enc_ftrs)\n",
        "        return enc_ftrs\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, enc_chs=(3,64,128,256,512,1024), dec_chs=(1024, 512, 256, 128, 64), num_class=3, retain_dim=False, out_sz=(572,572)):\n",
        "        super().__init__()\n",
        "        self.encoder     = Encoder(enc_chs)\n",
        "        self.decoder     = Decoder(dec_chs)\n",
        "        self.head        = nn.Conv2d(dec_chs[-1], num_class, 1)\n",
        "        self.retain_dim  = retain_dim\n",
        "        self.out_sz      = out_sz\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc_ftrs = self.encoder(x)\n",
        "        out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
        "        out      = self.head(out)\n",
        "        if self.retain_dim:\n",
        "            out = F.interpolate(out, self.out_sz)\n",
        "        return out\n",
        "\n",
        "class ComplexDiffusionModel(nn.Module):\n",
        "    def __init__(self, timesteps=10, beta_start=0.0001, beta_end=0.0005):\n",
        "        super(ComplexDiffusionModel, self).__init__()\n",
        "        self.timesteps = timesteps\n",
        "\n",
        "        # Define the variance schedule (beta)\n",
        "        self.betas = torch.linspace(beta_start, beta_end, timesteps)\n",
        "\n",
        "        # Precompute alpha and alpha_cumprod\n",
        "        self.alphas = 1.0 - self.betas\n",
        "        self.alpha_cumprod = torch.cumprod(self.alphas, dim=0)\n",
        "\n",
        "        # Define the UNet model\n",
        "        self.unet = UNet()\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        return self.unet(x)\n",
        "\n",
        "    def forward_process(self, x0, t):\n",
        "        alpha_t = self.alpha_cumprod[t].view(-1, 1, 1, 1)\n",
        "        std_dev = torch.sqrt(1.0 - alpha_t)\n",
        "        noise = torch.randn_like(x0) * 0.1  # Scaled down noise\n",
        "        x_t = alpha_t.sqrt() * x0 + std_dev * noise\n",
        "        return x_t, noise\n",
        "\n",
        "    def reverse_process(self, xt, t):\n",
        "        predicted_noise = self.forward(xt, t)\n",
        "        alpha_t = self.alpha_cumprod[t].view(-1, 1, 1, 1)\n",
        "        alpha_prev = self.alpha_cumprod[t - 1].view(-1, 1, 1, 1)\n",
        "\n",
        "        xt_predicted = (xt - torch.sqrt(1.0 - alpha_t) * predicted_noise) / torch.sqrt(alpha_t)\n",
        "        x_prev = torch.sqrt(alpha_prev) * xt_predicted + torch.sqrt(1 - alpha_prev) * predicted_noise\n",
        "\n",
        "        return x_prev\n",
        "\n",
        "# Helper function to convert tensor to PIL image\n",
        "def tensor_to_pil_image(tensor):\n",
        "    tensor = (tensor - tensor.min()) / (tensor.max() - tensor.min())  # Normalize to [0, 1]\n",
        "    tensor = tensor.mul(255).byte()\n",
        "    tensor = tensor.permute(1, 2, 0).cpu().numpy()\n",
        "    return Image.fromarray(tensor)\n",
        "\n",
        "# Training function\n",
        "def train(model, dataloader, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        for batch_idx, x0 in enumerate(dataloader):\n",
        "            x0 = x0.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            t = torch.randint(0, model.timesteps, (x0.size(0),), device=x0.device)\n",
        "            xt, noise = model.forward_process(x0, t)\n",
        "            predicted_noise = model.reverse_process(xt, t)\n",
        "            loss = loss_fn(noise, predicted_noise)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate the loss for this epoch\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Save the original and enhanced images\n",
        "            if (epoch + 1) % 20 == 0 and batch_idx == 0:\n",
        "                with torch.no_grad():\n",
        "                    model.eval()\n",
        "                    x0_sample = tensor_to_pil_image(x0[0])\n",
        "                    xt_sample = tensor_to_pil_image(xt[0])\n",
        "\n",
        "                    original_image_path = os.path.join(enhance_folder, f'epoch_{epoch}_batch_{batch_idx}_original.png')\n",
        "                    enhanced_image_path = os.path.join(enhance_folder, f'epoch_{epoch}_batch_{batch_idx}_enhanced.png')\n",
        "\n",
        "                    # Ensure the directory exists\n",
        "                    os.makedirs(enhance_folder, exist_ok=True)\n",
        "\n",
        "                    x0_sample.save(original_image_path)\n",
        "                    xt_sample.save(enhanced_image_path)\n",
        "\n",
        "                    print(f'Saved original image to {original_image_path}')\n",
        "                    print(f'Saved enhanced image to {enhanced_image_path}')\n",
        "\n",
        "        # Print the average loss for this epoch after every 200 epochs\n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            avg_epoch_loss = epoch_loss / len(dataloader)\n",
        "            print(f'Epoch {epoch + 1}, Average Loss: {avg_epoch_loss}')\n",
        "\n",
        "# Usage\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = ComplexDiffusionModel().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_fn = nn.MSELoss()\n",
        "enhance_folder = '/content/drive/MyDrive/enchance'\n",
        "\n",
        "# Assume `dataloader` is defined and provides data\n",
        "train(model, dataloader, epochs=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "U-VRQR2uXeuO",
        "outputId": "6b636d40-6f0d-412e-acaf-e0e542e9624d"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'CustomImageDataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-78946ce7bf6b>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Create DataLoader for new images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mnew_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomImageDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_images_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mnew_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CustomImageDataset' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "\n",
        "# Assuming CustomImageDataset, ComplexDiffusionModel, and tensor_to_pil_image are defined\n",
        "\n",
        "# Path to the directory containing the new images\n",
        "new_images_dir = '/path/to/new/images'\n",
        "enhanced_output_dir = '/path/to/save/enhanced/images'\n",
        "\n",
        "# Create DataLoader for new images\n",
        "new_dataset = CustomImageDataset(new_images_dir)\n",
        "new_dataloader = DataLoader(new_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Load the trained model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = ComplexDiffusionModel().to(device)\n",
        "model.load_state_dict(torch.load('/path/to/saved/model_state_dict.pth'))  # Adjust path\n",
        "model.eval()\n",
        "\n",
        "# Ensure the output directory exists\n",
        "os.makedirs(enhanced_output_dir, exist_ok=True)\n",
        "\n",
        "# Enhance and save new images\n",
        "with torch.no_grad():\n",
        "    for idx, image_tensor in enumerate(new_dataloader):\n",
        "        image_tensor = image_tensor.to(device)\n",
        "        t = torch.randint(0, model.timesteps, (image_tensor.size(0),), device=image_tensor.device)\n",
        "        xt, _ = model.forward_process(image_tensor, t)\n",
        "        enhanced_image_tensor = model.reverse_process(xt, t)\n",
        "\n",
        "        # Convert to PIL image and save\n",
        "        enhanced_image = tensor_to_pil_image(enhanced_image_tensor[0])\n",
        "        enhanced_image_path = os.path.join(enhanced_output_dir, f'enhanced_{idx}.png')\n",
        "        enhanced_image.save(enhanced_image_path)\n",
        "\n",
        "        print(f'Saved enhanced image to {enhanced_image_path}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtOHegL1zXCi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}